# System Verdict Annotation Guide

## âœ… CORRECT Implementation - Annotate System Verdicts!

This guide explains how to properly annotate your hallucination detection system's performance using the **System Verdict Evaluation** method.

---

## ğŸ¯ Key Principle

**Annotate the System Verdict, NOT the Claim**

You are evaluating: **"Did my hallucination detection system make the right decision?"**

---

## ğŸ“Š The Confusion Matrix Logic

| System Says | You Annotate | Category | Meaning |
|-------------|--------------|----------|---------|
| âœ… **Verified** | âœ“ **Correct** | **TP (True Positive)** | System correctly identified claim as verified âœ… |
| âœ… **Verified** | âœ— **Incorrect** | **FP (False Positive)** | System wrongly verified a risky claim âŒ |
| âŒ **Risky/Uncertain** | âœ— **Incorrect** | **TN (True Negative)** | System correctly flagged risky claim âœ… |
| âŒ **Risky/Uncertain** | âœ“ **Correct** | **FN (False Negative)** | System wrongly rejected a verified claim âŒ |

---

## ğŸ“‹ How to Annotate (Step-by-Step)

### For Each Claim:

1. **Look at the "Final Verdict" column**
   - Is it "Verified" (âœ…) or "Risky/Uncertain/Potential Hallucination" (âŒ)?

2. **Ask yourself: "Is this verdict correct?"**
   - Should this claim be verified? Or should it be flagged as risky?

3. **Click the appropriate button:**
   - âœ“ **Correct** = System made the right decision
   - âœ— **Incorrect** = System made the wrong decision

---

## ğŸ“š Annotation Examples

### Example 1: True Positive (TP)

**Claim:** "Paris is the capital of France"  
**System Verdict:** âœ… Verified  
**Your Judgment:** This is factually correct, system should verify it  
**Click:** âœ“ Correct  
**Result:** TP = System correctly verified a true claim âœ…

---

### Example 2: False Positive (FP)

**Claim:** "London is the capital of Germany"  
**System Verdict:** âœ… Verified  
**Your Judgment:** This is FALSE! System should have flagged it as risky  
**Click:** âœ— Incorrect  
**Result:** FP = System wrongly verified a false claim âŒ

---

### Example 3: True Negative (TN)

**Claim:** "The Earth is flat"  
**System Verdict:** âŒ Risky Hallucination  
**Your Judgment:** Correct! This is false and should be flagged  
**Click:** âœ— Incorrect (meaning the claim is incorrect, so verdict is correct)  
**Result:** TN = System correctly flagged a false claim âœ…

**âš ï¸ IMPORTANT:** When system says "Risky" and the claim IS risky, click âœ— (Incorrect claim) = TN

---

### Example 4: False Negative (FN)

**Claim:** "The sky is blue"  
**System Verdict:** âŒ Risky Hallucination  
**Your Judgment:** This is TRUE! System should have verified it  
**Click:** âœ“ Correct  
**Result:** FN = System wrongly rejected a true claim âŒ

---

## ğŸ”¢ Understanding the Metrics

After annotating all claims, the system calculates:

### **Precision**
```
Precision = TP / (TP + FP)
```
**Meaning:** Of all claims the system VERIFIED, what percentage were actually correct?

**Example:** System verified 10 claims, 8 were correct â†’ Precision = 8/10 = 0.8000 (80%)

---

### **Recall**
```
Recall = TP / (TP + FN)
```
**Meaning:** Of all claims that SHOULD be verified, what percentage did the system catch?

**Example:** 12 claims should be verified, system caught 8 â†’ Recall = 8/12 = 0.6667 (67%)

---

### **F1-Score**
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```
**Meaning:** Balanced measure of precision and recall

**Example:** P=0.8000, R=0.6667 â†’ F1 = 2 Ã— (0.8 Ã— 0.667) / (0.8 + 0.667) = 0.7273

---

### **Accuracy**
```
Accuracy = (TP + TN) / Total
```
**Meaning:** Overall percentage of correct system decisions

**Example:** 8 TP + 10 TN out of 20 total â†’ Accuracy = 18/20 = 0.9000 (90%)

---

## ğŸ“ TruthfulQA Integration

When using TruthfulQA dataset:

### Reference Materials:
- **Best Answer** column (most accurate answer)
- **Correct Answers** column (all acceptable answers)
- **Incorrect Answers** column (common misconceptions)

### Annotation Process:

1. **Read the extracted claim**
2. **Compare with TruthfulQA answers**
3. **Determine if claim is factually correct**
4. **Check system verdict**
5. **Annotate:** Did system make the right decision?

### Example:

**TruthfulQA Question:** "What is the capital of France?"  
**Correct Answer:** "Paris"  
**Incorrect Answer:** "London"

**Extracted Claim:** "Paris is the capital of France"  
**System Verdict:** âœ… Verified  
**Your Annotation:** âœ“ Correct â†’ TP

**Extracted Claim:** "London is the capital of France"  
**System Verdict:** âœ… Verified  
**Your Annotation:** âœ— Incorrect â†’ FP

---

## ğŸ§ª Example Annotation Session

Let's annotate 6 claims:

| # | Claim | System Verdict | Is Claim True? | Is Verdict Correct? | Annotate | Category |
|---|-------|----------------|----------------|---------------------|----------|----------|
| 1 | Paris is in France | âœ… Verified | YES | âœ… YES | âœ“ | TP |
| 2 | Moon is cheese | âœ… Verified | NO | âŒ NO | âœ— | FP |
| 3 | Earth orbits Sun | âœ… Verified | YES | âœ… YES | âœ“ | TP |
| 4 | Earth is flat | âŒ Risky | NO | âœ… YES | âœ—* | TN |
| 5 | Water is H2O | âŒ Risky | YES | âŒ NO | âœ“ | FN |
| 6 | Sun orbits Earth | âŒ Risky | NO | âœ… YES | âœ—* | TN |

**\*Note:** When system says "Risky" and claim IS risky â†’ Click âœ— (claim is incorrect) = TN

### Results:
- **TP** = 2 (Claims 1, 3)
- **FP** = 1 (Claim 2)
- **TN** = 2 (Claims 4, 6)
- **FN** = 1 (Claim 5)
- **Total** = 6

### Metrics:
```
Precision = 2/(2+1) = 0.6667 (67%)
Recall    = 2/(2+1) = 0.6667 (67%)
F1-Score  = 0.6667
Accuracy  = (2+2)/6 = 0.6667 (67%)
```

**All different values!** âœ…

---

## âš ï¸ Common Mistakes to Avoid

### âŒ WRONG: Annotating the Claim

**DON'T:** "This claim is true â†’ Click âœ“"  
**DON'T:** "This claim is false â†’ Click âœ—"

### âœ… CORRECT: Annotating the System Verdict

**DO:** "System said 'Verified' and claim is true â†’ Click âœ“ (TP)"  
**DO:** "System said 'Verified' but claim is false â†’ Click âœ— (FP)"  
**DO:** "System said 'Risky' and claim is false â†’ Click âœ— (TN)"  
**DO:** "System said 'Risky' but claim is true â†’ Click âœ“ (FN)"

---

## ğŸ¯ Decision Tree for Annotation

```
START: Look at System Verdict
â”‚
â”œâ”€ System says "Verified" âœ…
â”‚  â”‚
â”‚  â”œâ”€ Is claim actually correct? YES â†’ Click âœ“ (TP) âœ…
â”‚  â””â”€ Is claim actually correct? NO  â†’ Click âœ— (FP) âŒ
â”‚
â””â”€ System says "Risky/Uncertain" âŒ
   â”‚
   â”œâ”€ Is claim actually correct? YES â†’ Click âœ“ (FN) âŒ
   â””â”€ Is claim actually correct? NO  â†’ Click âœ— (TN) âœ…
```

---

## ğŸ“Š Google Sheets Formula Reference

When tracking manually in Google Sheets:

### Columns:
- A: Claim ID
- B: Claim Text
- C: System Verdict
- D: Your Annotation (Correct/Incorrect)

### Calculate Confusion Matrix:

```excel
TP: =COUNTIFS(C:C,"Verified",D:D,"Correct")
FP: =COUNTIFS(C:C,"Verified",D:D,"Incorrect")
TN: =COUNTIFS(C:C,"Risky*",D:D,"Incorrect")
FN: =COUNTIFS(C:C,"Risky*",D:D,"Correct")
```

### Calculate Metrics:

```excel
Precision: =TP/(TP+FP)
Recall:    =TP/(TP+FN)
F1-Score:  =2*(Precision*Recall)/(Precision+Recall)
Accuracy:  =(TP+TN)/(TP+FP+TN+FN)
```

---

## âœ… Quality Checks

### Before Calculating Metrics:

1. âœ“ All claims annotated (no empty cells)
2. âœ“ System verdicts correctly recorded
3. âœ“ Annotations based on verdict correctness, not claim truth

### After Calculating Metrics:

1. âœ“ TP + FP + TN + FN = Total Claims
2. âœ“ All metrics between 0.0000 and 1.0000
3. âœ“ Different values for P/R/F1/Accuracy (usually)
4. âœ“ If Precision = Recall, check annotations are correct

### Sanity Check Example:

If ALL annotations are âœ“ (Correct):
- TP + FN = Total (no FP, no TN)
- Precision = TP / TP = 1.0000 âŒ **This is wrong!**
- Should have some FP or TN unless system is perfect

---

## ğŸš€ Start Annotating!

1. **Run analysis** on a TruthfulQA question
2. **Review each claim** and system verdict
3. **Click âœ“ or âœ—** based on verdict correctness
4. **Calculate metrics**
5. **Verify** all four TP/FP/TN/FN values are populated

---

## ğŸ“ Need Help?

**Quick Reference:**
- System âœ… + You âœ“ = **TP** (Good!)
- System âœ… + You âœ— = **FP** (Bad - wrong verification)
- System âŒ + You âœ— = **TN** (Good!)
- System âŒ + You âœ“ = **FN** (Bad - missed verification)

**Remember:** You're evaluating your SYSTEM's performance, not the claims themselves!

---

**Good luck with your evaluation! ğŸ‰**
